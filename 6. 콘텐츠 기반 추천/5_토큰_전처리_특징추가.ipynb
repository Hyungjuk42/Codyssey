{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "def preprocess_synopsis(synopsis, genre, actors, genre_weight=1, actor_weight=1):\n",
    "    \"\"\"\n",
    "    영화 시놉시스, 장르, 배우 정보를 전처리하는 함수.\n",
    "\n",
    "    Args:\n",
    "        synopsis: 시놉시스 텍스트.\n",
    "        genre: 장르 (문자열).\n",
    "        actors: 배우 리스트.\n",
    "        genre_weight: 장르 가중치 (몇 번 반복할지).\n",
    "        actor_weight: 배우 가중치.\n",
    "\n",
    "    Returns:\n",
    "        전처리된 토큰 리스트.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. 특수문자 제거 (., \", ' 제외)\n",
    "    synopsis = re.sub(r\"[^가-힣A-Za-z0-9.,\\\"']+\", \" \", synopsis)\n",
    "\n",
    "    # 2. 불용어 제거 (직접 정의)\n",
    "    stopwords = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에게\", \"의\", \"와\", \"과\", \"만\", \"도\", \"에\", \"고\", \"다\", \"것\", \"로\", \"게\", \"두\", \"들\"]\n",
    "\n",
    "    # 3. 명사 추출 (KoNLPy 사용)\n",
    "    nouns = okt.nouns(synopsis)  # 명사만 추출\n",
    "    filtered_nouns = [noun for noun in nouns if noun not in stopwords] # 불용어 제거\n",
    "\n",
    "    # 4. 장르, 배우 정보 추가 (가중치 적용)\n",
    "    genre_tokens = [genre.lower()] * genre_weight\n",
    "    actor_tokens = [actor.lower() for actor in actors] * actor_weight\n",
    "\n",
    "    # 명사 토큰, 장르 토큰, 배우 토큰을 모두 합칩니다.\n",
    "    all_tokens = filtered_nouns + genre_tokens + actor_tokens\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "\n",
    "\n",
    "def create_tfidf_matrix(documents_tokens):\n",
    "    \"\"\"\n",
    "    문서별 토큰 리스트로부터 TF-IDF 행렬을 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "      documents_tokens: 문서별 토큰 리스트의 리스트 (예: [[\"나\", \"영화\"], [\"영화\", \"재미\"]])\n",
    "\n",
    "    Returns:\n",
    "        TF-IDF 행렬 (NumPy 배열), 단어 사전 (dict)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. DTM 및 단어 사전 생성\n",
    "    def create_dtm(documents_tokens):\n",
    "      # 1. 단어 사전 만들기\n",
    "      word_dict = defaultdict(lambda: len(word_dict))\n",
    "\n",
    "      # 2. DTM 생성\n",
    "      dtm = []\n",
    "      for tokens in documents_tokens:\n",
    "          term_frequency = {}  # 문서 내 단어 빈도\n",
    "          for token in tokens:\n",
    "              term_frequency[word_dict[token]] = term_frequency.get(word_dict[token], 0) + 1\n",
    "          dtm.append(term_frequency) # dtm에 추가\n",
    "\n",
    "      return dtm, word_dict\n",
    "\n",
    "    dtm, word_dict = create_dtm(documents_tokens)\n",
    "\n",
    "    # 2. TF 계산 (DTM을 NumPy 배열로 변환)\n",
    "    num_docs = len(documents_tokens)\n",
    "    num_words = len(word_dict)\n",
    "    tf_matrix = np.zeros((num_docs, num_words))\n",
    "    for i, doc_freq in enumerate(dtm):\n",
    "        for word_id, freq in doc_freq.items():\n",
    "            tf_matrix[i, word_id] = freq  # 문서 내 단어 빈도 (TF)\n",
    "\n",
    "    # 3. DF 계산\n",
    "    df = np.zeros(num_words)  # 각 단어별 문서 빈도\n",
    "    for word_id in range(num_words):\n",
    "      for doc_freq in dtm:\n",
    "        if word_id in doc_freq:\n",
    "          df[word_id] += 1\n",
    "\n",
    "\n",
    "    # 4. IDF 계산\n",
    "    idf = np.log(num_docs / (1 + df))\n",
    "\n",
    "    # 5. TF-IDF 계산\n",
    "    tfidf_matrix = tf_matrix * idf  # NumPy broadcasting 활용\n",
    "\n",
    "    return tfidf_matrix, word_dict\n",
    "\n",
    "# 영화 데이터 (시놉시스, 장르, 배우)\n",
    "movies = [\n",
    "    {\n",
    "        \"synopsis\": \"\"\"\n",
    "        \"나한테 별로 고마워하지 않아도 돼요\" 까칠한 어른 윤서\n",
    "        \"한 번 쯤은 자기를 믿어주는 사람이 있으면 좋잖아요\" 꿈 없는 청년 수찬\n",
    "\n",
    "        시청 정기간행물의 인터뷰어 '윤서'에게 사람의 온기는 한여름의 습하고 불쾌한 더위 같은 것.\n",
    "        그러던 어느 날, 청년 배달원 '수찬'과 실랑이를 벌이고 만다.\n",
    "        이후 인터뷰 자리에서 우연찮게 다시 만나게 되는데...\n",
    "\n",
    "        윤서와 수찬, 두 사람의 불편한 만남은 조금씩 서로를 건드린다.\n",
    "        \"\"\",\n",
    "        \"genre\": \"드라마\",\n",
    "        \"actors\": [\"임선우\", \"김명찬\", \"이장유\", \"박현숙\"]\n",
    "    },\n",
    "    {\n",
    "        \"synopsis\": \"\"\"\n",
    "        \"선생님, 저랑 사귀실래요?\" 적극적인 어른 민주\n",
    "        \"꺼져\" 철벽 많은 급식 윤서\n",
    "\n",
    "        윤서는 학교에서 학생들에게 인기가 매우 많은 선생님이다.\n",
    "        어느 날, 윤서는 민주로부터 고백을 받게 된다.\n",
    "        하지만 윤서는 민주를 거절한다.\n",
    "\n",
    "        윤서와 민주, 두 사람의 아슬아슬한 만남은 계속된다.\n",
    "        \"\"\",\n",
    "        \"genre\": \"로맨스\",\n",
    "        \"actors\": [\"김민주\", \"박서준\", \"이도현\"]\n",
    "    },\n",
    "    {\n",
    "       \"synopsis\": \"\"\"\n",
    "        1919년, 3.1 운동 이후 봉오동 전투에서 승리한 독립군의 이야기를 그린 영화.\n",
    "        \"\"\",\n",
    "        \"genre\": \"액션\",\n",
    "        \"actors\": [\"유해진\", \"류준열\", \"조우진\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 가중치 조합\n",
    "genre_weights = [1, 3, 5]  # 장르 가중치 후보\n",
    "actor_weights = [1, 3, 5]  # 배우 가중치 후보\n",
    "\n",
    "\n",
    "best_similarity = -1  # 가장 낮은 유사도를 -1로 초기화\n",
    "best_genre_weight = 1\n",
    "best_actor_weight = 1\n",
    "best_recommended_movies = []\n",
    "\n",
    "\n",
    "for genre_weight in genre_weights:\n",
    "    for actor_weight in actor_weights:\n",
    "        # 각 영화별 전처리된 토큰 리스트 생성\n",
    "        documents_tokens = [\n",
    "            preprocess_synopsis(movie[\"synopsis\"], movie[\"genre\"], movie[\"actors\"], genre_weight, actor_weight)\n",
    "            for movie in movies\n",
    "        ]\n",
    "        # TF-IDF 행렬 생성\n",
    "        tfidf_matrix, word_dict = create_tfidf_matrix(documents_tokens)\n",
    "\n",
    "        # 코사인 유사도 행렬 계산 (직접 구현)\n",
    "        num_movies = len(movies)\n",
    "        similarity_matrix = np.zeros((num_movies, num_movies))\n",
    "\n",
    "        for i in range(num_movies):\n",
    "            for j in range(i + 1, num_movies):\n",
    "                similarity = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])\n",
    "                similarity_matrix[i, j] = similarity\n",
    "                similarity_matrix[j, i] = similarity  # 대칭 행렬\n",
    "\n",
    "        # 0번 영화와 다른 영화들 간의 유사도\n",
    "        similarities = similarity_matrix[0, 1:]  # 0번 영화와 다른 영화(1,2) 간 유사도\n",
    "\n",
    "        # 현재 가중치 조합에서의 평균 유사도 (0번 영화와 다른 영화 간 유사도의 평균)\n",
    "        avg_similarity = np.mean(similarities)\n",
    "\n",
    "        if avg_similarity > best_similarity:\n",
    "            best_similarity = avg_similarity\n",
    "            best_genre_weight = genre_weight\n",
    "            best_actor_weight = actor_weight\n",
    "            best_recommended_movies = similarities\n",
    "\n",
    "print(f\"가장 좋은 유사도: {best_similarity}\")\n",
    "print(f\"최적 장르 가중치: {best_genre_weight}\")\n",
    "print(f\"최적 배우 가중치: {best_actor_weight}\")\n",
    "\n",
    "# 추천 영화 인덱스 정렬\n",
    "recommended_indices = np.argsort(best_recommended_movies)[::-1] # 내림차순 정렬\n",
    "print(f\"추천 영화 (0번 영화 기준): {[movies[i+1]['genre'] for i in recommended_indices]}\") # 인덱스 1, 2에 해당하는 영화 제목 출력\n",
    "print(best_recommended_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**주요 변경 사항:**\n",
    "\n",
    "1.  **`preprocess_synopsis` 함수:**\n",
    "    *   `genre_weight`, `actor_weight` 매개변수를 추가하여 장르와 배우 정보를 몇 번 반복해서 토큰 리스트에 추가할지 결정합니다.\n",
    "    *   `okt.nouns(synopsis)`: KoNLPy의 `Okt` 형태소 분석기를 사용하여 명사만 추출합니다.\n",
    "    *   불용어 제거를 명사에도 적용합니다.\n",
    "    *   장르와 배우 정보를 소문자로 변환하여 추가합니다.\n",
    "\n",
    "2.  **`create_tfidf_matrix` 함수:**\n",
    "    *   TfidfVectorizer를 사용하지 않고 직접 구현합니다.\n",
    "\n",
    "3.  **메인 로직:**\n",
    "    *   여러 `genre_weight`와 `actor_weight` 조합을 시도하면서 가장 좋은 유사도 점수를 보이는 조합을 찾습니다.\n",
    "    *   `cosine_similarity` (직접 구현)를 사용하여 유사도 행렬 계산.\n",
    "    *   0번 영화와 다른 영화들 간의 유사도를 기준으로 가장 좋은 가중치를 찾고, 해당 가중치에서의 추천 영화를 출력합니다.\n",
    "\n",
    "**실행 결과 및 해석:**\n",
    "\n",
    "*   코드를 실행하면 다양한 가중치 조합에 따른 유사도 점수와 추천 영화 목록이 출력됩니다.\n",
    "*   가장 좋은 유사도 점수는 0번 영화와 다른 영화들 간의 평균 코사인 유사도를 기준으로 합니다. (값이 높을수록 좋음).\n",
    "*   최적의 가중치는 해당 점수를 최대화하는 장르 가중치와 배우 가중치입니다.\n",
    "*  `recommended_indices`는 유사도가 높은 순으로 정렬된 인덱스를 나타냅니다.\n",
    "\n",
    "**가중치 튜닝:**\n",
    "\n",
    "*   현재 코드는 간단한 그리드 탐색(grid search) 방식으로 가중치를 튜닝합니다.\n",
    "*   더 정교한 튜닝을 위해서는 교차 검증(cross-validation)과 함께 다른 최적화 알고리즘(예: 베이지안 최적화)을 사용할 수 있습니다.\n",
    "\n",
    "**참고:**\n",
    "\n",
    "*   실제 영화 추천 시스템에서는 훨씬 더 많은 영화 데이터를 사용하고, 사용자-영화 상호작용 데이터(예: 평점)를 함께 고려하는 협업 필터링(collaborative filtering) 등의 고급 기법을 사용합니다.\n",
    "*   이 코드는 콘텐츠 기반 추천(content-based filtering)의 기본적인 예시를 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoNLPy (코엔엘파이)\n",
    "\n",
    "**KoNLPy**는 한국어 정보 처리를 위한 파이썬 패키지입니다. 형태소 분석, 품사 태깅, 구문 분석 등 다양한 자연어 처리 기능을 제공합니다. 특히, 한국어는 영어와 달리 형태소(의미를 가지는 가장 작은 단위) 분석이 중요하기 때문에 KoNLPy는 한국어 텍스트 분석에 필수적인 도구입니다.\n",
    "\n",
    "**주요 기능:**\n",
    "\n",
    "*   **형태소 분석기:** KoNLPy는 여러 가지 형태소 분석기를 내장하고 있습니다.\n",
    "    *   **Okt (Open Korean Text, 구 Twitter):** 속도가 빠르고, 신조어나 비표준어 처리 능력이 좋습니다.\n",
    "    *   **Kkma (꼬꼬마):** 분석 품질이 좋지만, 속도가 상대적으로 느립니다.\n",
    "    *   **Komoran (코모란):** 오탈자에 강하고, 사용자 사전 추가 기능이 있습니다.\n",
    "    *   **Hannanum (한나눔):** KAIST에서 개발한 형태소 분석기입니다.\n",
    "    *   **Mecab (메카브):** 일본어 형태소 분석기를 한국어에 맞게 수정한 것으로, 속도가 매우 빠릅니다. (별도 설치 필요)\n",
    "*   **품사 태깅 (POS Tagging):** 각 형태소에 품사를 태깅합니다. (예: 명사, 동사, 형용사 등)\n",
    "*   **명사 추출:** 텍스트에서 명사만 추출합니다.\n",
    "*   **구문 분석:** 문장의 구조를 분석합니다. (KoNLPy에서 직접 지원하지는 않지만, 다른 라이브러리와 함께 사용 가능)\n",
    "*   **말뭉치 (Corpus):** KoNLPy는 한국어 말뭉치(텍스트 데이터)에 접근할 수 있는 기능을 제공합니다. (예: 대한민국 헌법, 국회 의사록 등)\n",
    "\n",
    "**설치:**\n",
    "\n",
    "```bash\n",
    "pip install konlpy\n",
    "```\n",
    "\n",
    "**간단한 사용 예시 (Okt):**\n",
    "\n",
    "```python\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "text = \"한국어 자연어 처리는 재미있습니다.\"\n",
    "\n",
    "# 형태소 분석\n",
    "morphs = okt.morphs(text)\n",
    "print(\"형태소:\", morphs)\n",
    "\n",
    "# 품사 태깅\n",
    "pos = okt.pos(text)\n",
    "print(\"품사 태깅:\", pos)\n",
    "\n",
    "# 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "print(\"명사:\", nouns)\n",
    "```\n",
    "\n",
    "**출력:**\n",
    "\n",
    "```\n",
    "형태소: ['한국어', '자연어', '처리', '는', '재미있습니다', '.']\n",
    "품사 태깅: [('한국어', 'Noun'), ('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('재미있습니다', 'Adjective'), ('.', 'Punctuation')]\n",
    "명사: ['한국어', '자연어', '처리']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TfidfVectorizer\n",
    "\n",
    "**TfidfVectorizer**는 텍스트 데이터를 TF-IDF (Term Frequency-Inverse Document Frequency) 값으로 변환하는 sklearn(사이킷런)의 클래스입니다. TF-IDF는 텍스트 마이닝에서 단어의 중요도를 나타내는 데 사용되는 가중치입니다.\n",
    "\n",
    "**TF-IDF:**\n",
    "\n",
    "*   **TF (Term Frequency):** 단어가 문서 내에서 얼마나 자주 나타나는지를 나타냅니다.\n",
    "*   **IDF (Inverse Document Frequency):** 단어가 여러 문서에서 얼마나 드물게 나타나는지를 나타냅니다.\n",
    "*   **TF-IDF:** TF와 IDF를 곱한 값으로, 특정 문서에서 자주 등장하고 다른 문서에서는 드물게 등장하는 단어에 높은 가중치를 부여합니다.\n",
    "\n",
    "**TfidfVectorizer의 주요 기능:**\n",
    "\n",
    "*   **텍스트 전처리:**\n",
    "    *   소문자 변환\n",
    "    *   토큰화 (단어 분리) - 기본적으로 공백 기준\n",
    "    *   불용어(stop words) 제거 (영어 불용어 기본 제공, 사용자 정의 가능)\n",
    "*   **TF-IDF 계산:** 각 문서의 각 단어에 대한 TF-IDF 값을 계산하여 행렬 형태로 반환합니다.\n",
    "*   **희소 행렬(sparse matrix):**  결과 TF-IDF 행렬은 대부분의 값이 0인 희소 행렬로 표현됩니다. (메모리 효율적)\n",
    "*   **다양한 매개변수:**\n",
    "    *   `max_df`:  DF(document frequency)가 지정된 임계값보다 높은 단어는 무시합니다. (너무 자주 나타나는 단어 제거)\n",
    "    *   `min_df`: DF가 지정된 임계값보다 낮은 단어는 무시합니다. (너무 드물게 나타나는 단어 제거)\n",
    "    *   `max_features`: 최대 단어 수를 제한합니다.\n",
    "    *   `ngram_range`:  n-gram 범위를 지정합니다. (예: (1, 2)는 unigram과 bigram을 모두 사용)\n",
    "    *   `stop_words`: 불용어 리스트를 지정합니다. ('english' 또는 사용자 정의 리스트)\n",
    "    *   `tokenizer`: 사용자 정의 토큰화 함수를 지정합니다.\n",
    "    *   `norm`: 정규화 방법 ('l1', 'l2', None)을 지정합니다.\n",
    "\n",
    "**설치 (sklearn):**\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "\n",
    "**간단한 사용 예시:**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 문서 집합\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# TfidfVectorizer 객체 생성\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF 행렬 계산\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 단어 사전 (feature names)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"단어 사전:\", feature_names)\n",
    "\n",
    "# TF-IDF 행렬 (희소 행렬)\n",
    "print(\"TF-IDF 행렬:\\n\", tfidf_matrix)  # 희소 행렬 표현\n",
    "print(\"TF-IDF 행렬 (밀집 행렬):\\n\", tfidf_matrix.toarray()) # 밀집 행렬로 변환 (보기 편함)\n",
    "\n",
    "```\n",
    "\n",
    "**출력 (일부):**\n",
    "\n",
    "```\n",
    "단어 사전: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
    "TF-IDF 행렬:\n",
    "   (0, 7)\t0.46979138557992045\n",
    "  (0, 2)\t0.5802858236844359\n",
    "  (0, 3)\t0.38408524091481483\n",
    "  (0, 1)\t0.46979138557992045\n",
    "  (0, 6)\t0.281088674033753\n",
    "  ...\n",
    "TF-IDF 행렬 (밀집 행렬):\n",
    " [[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
    "  0.28108867 0.46979139 0.        ]\n",
    " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
    "  0.28108867 0.         0.        ]\n",
    " ...\n",
    "```\n",
    "\n",
    "**설명:**\n",
    "\n",
    "*   `vectorizer.fit_transform(documents)`:\n",
    "    *   `fit()`: 문서 집합(`documents`)으로부터 단어 사전을 만들고 IDF를 계산합니다.\n",
    "    *   `transform()`: 각 문서를 TF-IDF 벡터로 변환합니다.\n",
    "    *   `fit_transform()`: `fit()`과 `transform()`을 한 번에 수행합니다.\n",
    "*   `vectorizer.get_feature_names_out()`: 단어 사전(vocabulary)에 있는 단어들을 가져옵니다.\n",
    "*   `tfidf_matrix`: TF-IDF 행렬 (희소 행렬 형태)\n",
    "*   `tfidf_matrix.toarray()`: 희소 행렬을 밀집 행렬(dense matrix)로 변환 (보기 쉽게)\n",
    "\n",
    "**KoNLPy와 TfidfVectorizer 함께 사용:**\n",
    "\n",
    "KoNLPy의 형태소 분석기를 TfidfVectorizer의 `tokenizer` 매개변수에 지정하여 한국어 텍스트를 처리할 수 있습니다.\n",
    "\n",
    "```python\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return okt.nouns(text)  # 명사만 추출\n",
    "\n",
    "# TfidfVectorizer 객체 생성 (tokenizer 지정)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer)\n",
    "\n",
    "# ... (TF-IDF 행렬 계산 등) ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
