{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LDA (Latent Dirichlet Allocation, 잠재 디리클레 할당)\n",
    "\n",
    "**LDA (Latent Dirichlet Allocation)** 는 텍스트 문서 집합에서 숨겨진 토픽(주제)을 찾아내는 확률적 토픽 모델링 기법 중 하나입니다. LDA는 다음과 같은 가정 하에 작동합니다.\n",
    "\n",
    "*   **가정:**\n",
    "    *   각 문서는 여러 토픽의 혼합으로 구성되어 있다.\n",
    "    *   각 토픽은 여러 단어의 확률 분포로 나타낼 수 있다.\n",
    "    *   문서 내의 각 단어는 특정 토픽에 속하며, 그 토픽에 해당하는 단어 분포에서 생성된다.\n",
    "\n",
    "*   **목표:**\n",
    "    *   주어진 문서 집합에서 각 문서의 토픽 분포와 각 토픽의 단어 분포를 추정하는 것.\n",
    "\n",
    "**LDA의 작동 방식 (간단하게):**\n",
    "\n",
    "1.  **초기화:**\n",
    "    *   토픽의 수 (k)를 결정합니다. (하이퍼파라미터)\n",
    "    *   각 문서에 대해 랜덤하게 토픽을 할당합니다.\n",
    "    *   각 토픽에 대해 랜덤하게 단어를 할당합니다.\n",
    "\n",
    "2.  **반복 (깁스 샘플링):**\n",
    "    *   모든 문서의 모든 단어에 대해 다음을 반복합니다.\n",
    "        *   현재 단어의 토픽 할당을 제외하고, 다른 모든 단어의 토픽 할당과 문서의 토픽 분포, 토픽의 단어 분포를 기반으로 현재 단어의 새로운 토픽을 확률적으로 선택합니다. (샘플링)\n",
    "        *   새로운 토픽을 할당합니다.\n",
    "\n",
    "3.  **수렴:**\n",
    "    *   위의 반복 과정을 충분히 수행하면, 각 문서의 토픽 분포와 각 토픽의 단어 분포가 안정적인 상태(수렴)에 도달합니다.\n",
    "\n",
    "**결과:**\n",
    "\n",
    "*   **각 문서의 토픽 분포 (Document-Topic Distribution):** 각 문서가 어떤 토픽들로 구성되어 있는지를 나타내는 확률 분포 (예: 문서 1은 토픽 1에 70%, 토픽 2에 30% 속한다).\n",
    "*   **각 토픽의 단어 분포 (Topic-Word Distribution):** 각 토픽이 어떤 단어들로 구성되어 있는지를 나타내는 확률 분포 (예: 토픽 1은 \"영화\", \"감독\", \"배우\" 등의 단어가 높은 확률을 가진다).\n",
    "\n",
    "**LDA의 장점:**\n",
    "\n",
    "*   문서 집합의 잠재적인 토픽 구조를 자동으로 발견할 수 있습니다.\n",
    "*   각 문서와 토픽을 확률적으로 표현하여 불확실성을 다룰 수 있습니다.\n",
    "*   토픽 모델링 결과는 텍스트 분류, 추천 시스템, 정보 검색 등 다양한 분야에 활용될 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LDA vs. LSA\n",
    "\n",
    "| 특징         | LDA                                                                                              | LSA                                                                                                                                                                                                                                                                                                                                |\n",
    "| :----------- | :----------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **기반**     | 확률적 모델 (Generative Model, 생성 모델),  디리클레 분포                                                 | 선형 대수 (Linear Algebra), 특이값 분해 (Singular Value Decomposition, SVD)                                                                                                                                                                                                                                           |\n",
    "| **목표**     | 문서 집합의 잠재 토픽 구조 발견,  각 문서의 토픽 분포 및 각 토픽의 단어 분포 추정                                    | 단어-문서 행렬의 차원 축소,  단어와 문서의 잠재 의미 공간 표현                                                                                                                                                                                                                                                                                             |\n",
    "| **결과**     | 각 문서의 토픽 분포 (확률), 각 토픽의 단어 분포 (확률)                                                              | 각 문서의 LSA 벡터, 각 단어의 LSA 벡터, 토픽-단어 행렬 (components_)                                                                                                                                                                                                                                                                      |\n",
    "| **해석**     | 확률 분포로 표현되어 직관적, 토픽의 의미를 단어 분포를 통해 해석                                                      | 토픽-단어 행렬 (components_)의 각 행이 토픽을 나타내지만, 각 차원(토픽)이 명확한 의미를 가지지 않을 수 있음                                                                                                                                                                                                                |\n",
    "| **계산**     | 반복적인 확률적 샘플링 (깁스 샘플링 등) 필요, 계산 비용이 높을 수 있음                                                    | SVD 계산 (일반적으로 한 번), 비교적 빠름                                                                                                                                                                                                                                                                                              |\n",
    "| **하이퍼파라미터** | 토픽 수 (k), 알파 (α, 문서의 토픽 분포 관련), 베타 (β, 토픽의 단어 분포 관련)                                         | 축소할 차원(토픽) 수                                                                                                                                                                                                                                                                                                                      |\n",
    "| **동의어/다의어** | 동의어는 같은 토픽에 속할 확률이 높음, 다의어는 문맥에 따라 다른 토픽에 할당될 수 있음                                        | 동의어는 비슷한 벡터를 가지게 됨, 다의어는 여러 의미가 혼합된 벡터를 가질 수 있음                                                                                                                                                                                                                                           |\n",
    "| **적용**     | 텍스트 분류, 추천 시스템, 정보 검색, 토픽 트렌드 분석 등                                                               | 텍스트 분류, 추천 시스템, 정보 검색, 문서 유사도 계산, 텍스트 데이터 시각화 등                                                                                                                                                                                                                                           |\n",
    "\n",
    "**요약:**\n",
    "\n",
    "*   LDA는 확률 기반 모델이고, LSA는 선형 대수 기반 모델입니다.\n",
    "*   LDA는 결과가 확률 분포로 표현되어 해석이 용이하지만, 계산 비용이 높을 수 있습니다.\n",
    "*   LSA는 계산이 빠르지만, 결과의 해석이 LDA만큼 직관적이지 않을 수 있습니다.\n",
    "\n",
    "어떤 방법을 사용할지는 데이터의 특성, 분석 목표, 계산 환경 등을 고려하여 결정해야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF 행렬과 LDA를 이용한 영화별 토픽 분류\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 이전 코드에서 tfidf_matrix, documents_tokens, word_dict 가져오기\n",
    "# (영화 데이터, preprocess_synopsis, create_tfidf_matrix 함수 등)\n",
    "# 예시 데이터 및 전처리 (위의 코드에서 가져옴)\n",
    "# 영화 데이터 (시놉시스, 장르, 배우)\n",
    "movies = [\n",
    "    {\n",
    "        \"synopsis\": \"\"\"\n",
    "        \"나한테 별로 고마워하지 않아도 돼요\" 까칠한 어른 윤서\n",
    "        \"한 번 쯤은 자기를 믿어주는 사람이 있으면 좋잖아요\" 꿈 없는 청년 수찬\n",
    "\n",
    "        시청 정기간행물의 인터뷰어 '윤서'에게 사람의 온기는 한여름의 습하고 불쾌한 더위 같은 것.\n",
    "        그러던 어느 날, 청년 배달원 '수찬'과 실랑이를 벌이고 만다.\n",
    "        이후 인터뷰 자리에서 우연찮게 다시 만나게 되는데...\n",
    "\n",
    "        윤서와 수찬, 두 사람의 불편한 만남은 조금씩 서로를 건드린다.\n",
    "        \"\"\",\n",
    "        \"genre\": \"드라마\",\n",
    "        \"actors\": [\"임선우\", \"김명찬\", \"이장유\", \"박현숙\"]\n",
    "    },\n",
    "    {\n",
    "        \"synopsis\": \"\"\"\n",
    "        \"선생님, 저랑 사귀실래요?\" 적극적인 어른 민주\n",
    "        \"꺼져\" 철벽 많은 급식 윤서\n",
    "\n",
    "        윤서는 학교에서 학생들에게 인기가 매우 많은 선생님이다.\n",
    "        어느 날, 윤서는 민주로부터 고백을 받게 된다.\n",
    "        하지만 윤서는 민주를 거절한다.\n",
    "\n",
    "        윤서와 민주, 두 사람의 아슬아슬한 만남은 계속된다.\n",
    "        \"\"\",\n",
    "        \"genre\": \"로맨스\",\n",
    "        \"actors\": [\"김민주\", \"박서준\", \"이도현\"]\n",
    "    },\n",
    "    {\n",
    "       \"synopsis\": \"\"\"\n",
    "        1919년, 3.1 운동 이후 봉오동 전투에서 승리한 독립군의 이야기를 그린 영화.\n",
    "        \"\"\",\n",
    "        \"genre\": \"액션\",\n",
    "        \"actors\": [\"유해진\", \"류준열\", \"조우진\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 가중치\n",
    "genre_weight = 3\n",
    "actor_weight = 3\n",
    "\n",
    "# 각 영화별 전처리된 토큰 리스트 생성\n",
    "documents_tokens = [\n",
    "    preprocess_synopsis(movie[\"synopsis\"], movie[\"genre\"], movie[\"actors\"], genre_weight, actor_weight)\n",
    "    for movie in movies\n",
    "]\n",
    "# TF-IDF 행렬 생성\n",
    "tfidf_matrix, word_dict = create_tfidf_matrix(documents_tokens)\n",
    "\n",
    "def perform_lda(tfidf_matrix, n_components=3):\n",
    "    \"\"\"\n",
    "    TF-IDF 행렬에 대해 LDA를 수행하는 함수.\n",
    "\n",
    "    Args:\n",
    "        tfidf_matrix: TF-IDF 행렬 (NumPy 배열 또는 희소 행렬).\n",
    "        n_components: 토픽의 수.\n",
    "\n",
    "    Returns:\n",
    "        LDA 모델, 문서-토픽 분포, 토픽-단어 분포.\n",
    "    \"\"\"\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
    "    doc_topic_dist = lda.fit_transform(tfidf_matrix) # 문서-토픽 분포\n",
    "    topic_word_dist = lda.components_ # 토픽-단어 분포.  normalize=True를 안하면 빈도수로 나옴.\n",
    "    return lda, doc_topic_dist, topic_word_dist\n",
    "\n",
    "# LDA 수행 (토픽 수 변경하면서 실험)\n",
    "n_components_list = [2, 3, 5] # 토픽의 수를 바꿔가면서 확인\n",
    "\n",
    "for n_components in n_components_list:\n",
    "    print(f\"\\n--- 토픽 수: {n_components} ---\")\n",
    "    lda, doc_topic_dist, topic_word_dist = perform_lda(tfidf_matrix, n_components=n_components)\n",
    "\n",
    "    # 3-1. fit_transform() 결과 출력 (문서-토픽 분포)\n",
    "    print(\"\\n문서-토픽 분포 (fit_transform 결과):\\n\", doc_topic_dist)\n",
    "\n",
    "    # 3-2. components_ 출력 (토픽-단어 분포)\n",
    "    print(\"\\n토픽-단어 분포 (components_):\\n\", topic_word_dist)\n",
    "\n",
    "    # 토픽별 주요 단어 확인\n",
    "    n_top_words = 5\n",
    "    feature_names = list(word_dict.keys())\n",
    "\n",
    "    for topic_idx, topic in enumerate(topic_word_dist):\n",
    "        print(f\"\\n토픽 #{topic_idx + 1}:\")\n",
    "        top_word_indices = topic.argsort()[::-1][:n_top_words]\n",
    "        top_words = [feature_names[i] for i in top_word_indices]\n",
    "        print(\" \".join(top_words))\n",
    "```\n",
    "\n",
    "**설명:**\n",
    "\n",
    "1.  **`LatentDirichletAllocation`:** sklearn의 `LatentDirichletAllocation` 클래스를 사용하여 LDA 모델을 생성합니다.\n",
    "2.  **`n_components`:** 토픽의 수를 지정합니다. (하이퍼파라미터)\n",
    "3.  **`random_state`:** 결과의 재현성을 위해 난수 생성기의 시드를 설정합니다.\n",
    "4.  **`lda.fit_transform(tfidf_matrix)`:**\n",
    "    *   `fit()`: TF-IDF 행렬(`tfidf_matrix`)을 입력으로 받아 LDA 모델을 학습시킵니다. (토픽-단어 분포, 문서-토픽 분포 추정)\n",
    "    *   `transform()`: 학습된 LDA 모델을 사용하여 각 문서를 토픽 분포로 변환합니다.\n",
    "    *    `fit_transform()`은 이 두 단계를 함께 수행하며, 각 문서에 대한 토픽 분포(document-topic distribution)를 반환합니다.\n",
    "5.  **`lda.components_`:** 학습된 LDA 모델의 토픽-단어 분포(topic-word distribution)를 반환합니다.\n",
    "6. **토픽 수 변경:** `n_components_list`에 있는 여러 토픽 수에 대해 LDA를 수행하고 결과를 확인합니다.\n",
    "7. **결과 출력:**\n",
    "    *   `doc_topic_dist`: 각 문서의 토픽 분포를 출력합니다.\n",
    "    *   `topic_word_dist`: 각 토픽의 단어 분포를 출력합니다.\n",
    "    *   토픽별 주요 단어를 출력하여 각 토픽의 의미를 파악합니다.\n",
    "\n",
    "**결과 해석:**\n",
    "\n",
    "*   **문서-토픽 분포 (`doc_topic_dist`):** 각 문서가 어떤 토픽들로 구성되어 있는지 확률적으로 보여줍니다. 예를 들어, 첫 번째 문서가 토픽 1에 0.8, 토픽 2에 0.2의 확률로 속한다면, 이 문서는 주로 토픽 1에 관한 내용을 담고 있다고 해석할 수 있습니다.\n",
    "*   **토픽-단어 분포 (`topic_word_dist`):** 각 토픽이 어떤 단어들로 구성되어 있는지 확률적으로 보여줍니다.  `components_`의 각 행은 토픽을 나타내고, 각 열은 단어를 나타냅니다.  셀의 값은 해당 토픽에서 해당 단어가 나타날 확률(또는 빈도)을 나타냅니다.\n",
    "*   **토픽별 주요 단어:** 각 토픽에서 가장 높은 확률(또는 빈도)을 가지는 단어를 보면 해당 토픽의 의미를 파악할 수 있습니다.\n",
    "\n",
    "**주의:** LDA는 확률적 모델이므로 실행할 때마다 결과가 약간씩 달라질 수 있습니다.  `random_state`를 고정하면 결과의 재현성을 확보할 수 있습니다.\n",
    "\n",
    "이 코드는 LDA를 사용하여 영화별 토픽을 분류하고, 각 토픽의 의미를 파악하는 방법을 보여줍니다. 토픽 수는 데이터와 분석 목적에 맞게 적절히 조절해야 합니다.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
