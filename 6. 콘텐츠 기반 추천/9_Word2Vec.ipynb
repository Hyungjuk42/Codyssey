{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word2Vec\n",
    "\n",
    "**Word2Vec**은 단어를 고정된 차원의 밀집 벡터(dense vector)로 표현하는 분산 표현(distributed representation) 방법 중 하나입니다. Word2Vec은 단어의 의미를 벡터 공간에 투영하여, 의미가 비슷한 단어들은 벡터 공간에서 가깝게 위치하도록 학습합니다.\n",
    "\n",
    "**Word2Vec의 핵심 아이디어:**\n",
    "\n",
    "*   **분포 가설 (Distributional Hypothesis):** \"비슷한 문맥에서 나타나는 단어는 비슷한 의미를 가진다.\"\n",
    "*   Word2Vec은 이 가설을 바탕으로, 대량의 텍스트 데이터(코퍼스)를 학습하여 단어 간의 관계를 파악하고, 각 단어를 벡터로 표현합니다.\n",
    "\n",
    "**Word2Vec의 장점:**\n",
    "\n",
    "*   **단어 간의 의미 관계 파악:** 벡터 연산을 통해 단어 간의 유사도, 관계 등을 파악할 수 있습니다. (예: \"왕\" - \"남자\" + \"여자\" ≈ \"여왕\")\n",
    "*   **차원 축소:** 고차원의 희소 벡터(예: one-hot encoding)를 저차원의 밀집 벡터로 변환하여 계산 효율성을 높입니다.\n",
    "*   **전이 학습 (Transfer Learning):** Word2Vec으로 학습된 임베딩(embedding)을 다른 자연어 처리 작업(예: 텍스트 분류, 감성 분석)의 초기값으로 사용하여 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "**Word2Vec의 종류:** CBOW, Skip-gram (아래에서 설명)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CBOW와 Skip-gram\n",
    "\n",
    "Word2Vec은 크게 두 가지 모델로 나뉩니다.\n",
    "\n",
    "**CBOW (Continuous Bag-of-Words):**\n",
    "\n",
    "*   **주변 단어 -> 중심 단어 예측:** 주변 단어(context words)들을 입력으로 받아 중심 단어(target word)를 예측하는 방식으로 학습합니다.\n",
    "*   **구조:**\n",
    "    *   입력층(Input Layer): 주변 단어들의 one-hot encoding 벡터\n",
    "    *   투영층(Projection Layer): 입력 벡터들을 평균내어 은닉층(hidden layer)에 투영 (가중치 행렬 곱)\n",
    "    *   출력층(Output Layer): 중심 단어를 예측하는 확률 분포 (softmax)\n",
    "*   **특징:**\n",
    "    *   학습 속도가 빠릅니다.\n",
    "    *   상대적으로 빈번하게 등장하는 단어에 대해 더 잘 학습합니다.\n",
    "\n",
    "**Skip-gram:**\n",
    "\n",
    "*   **중심 단어 -> 주변 단어 예측:** 중심 단어를 입력으로 받아 주변 단어들을 예측하는 방식으로 학습합니다.\n",
    "*   **구조:**\n",
    "    *   입력층: 중심 단어의 one-hot encoding 벡터\n",
    "    *   투영층: 입력 벡터를 은닉층에 투영 (가중치 행렬 곱)\n",
    "    *   출력층: 주변 단어들을 예측하는 확률 분포 (softmax)\n",
    "*   **특징:**\n",
    "    *   CBOW보다 학습 속도는 느리지만, 드물게 등장하는 단어나 특정 패턴을 가진 단어에 대해 더 잘 학습하는 경향이 있습니다.\n",
    "    *   일반적으로 CBOW보다 성능이 좋은 경우가 많습니다.\n",
    "\n",
    "**CBOW vs. Skip-gram 요약:**\n",
    "\n",
    "| 특징         | CBOW                                                               | Skip-gram                                                     |\n",
    "| :----------- | :----------------------------------------------------------------- | :------------------------------------------------------------ |\n",
    "| **예측 방향** | 주변 단어 -> 중심 단어                                               | 중심 단어 -> 주변 단어                                           |\n",
    "| **학습 속도** | 빠름                                                              | 느림                                                           |\n",
    "| **성능**      | 빈번한 단어에 강함                                                | 드문 단어, 특정 패턴에 강함, 일반적으로 CBOW보다 성능이 좋은 경우가 많음 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 영화 시놉시스 데이터와 Word2Vec을 이용한 영화별 토큰 평균 벡터 계산\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 이전 코드에서 preprocess_synopsis 함수와 영화 데이터(movies)를 가져와서 사용\n",
    "# 영화 데이터 (시놉시스, 장르, 배우)\n",
    "movies = [\n",
    "    {\n",
    "        \"synopsis\": \"\"\"\n",
    "        \"나한테 별로 고마워하지 않아도 돼요\" 까칠한 어른 윤서\n",
    "        \"한 번 쯤은 자기를 믿어주는 사람이 있으면 좋잖아요\" 꿈 없는 청년 수찬\n",
    "\n",
    "        시청 정기간행물의 인터뷰어 '윤서'에게 사람의 온기는 한여름의 습하고 불쾌한 더위 같은 것.\n",
    "        그러던 어느 날, 청년 배달원 '수찬'과 실랑이를 벌이고 만다.\n",
    "        이후 인터뷰 자리에서 우연찮게 다시 만나게 되는데...\n",
    "\n",
    "        윤서와 수찬, 두 사람의 불편한 만남은 조금씩 서로를 건드린다.\n",
    "        \"\"\",\n",
    "        \"genre\": \"드라마\",\n",
    "        \"actors\": [\"임선우\", \"김명찬\", \"이장유\", \"박현숙\"]\n",
    "    },\n",
    "    {\n",
    "        \"synopsis\": \"\"\"\n",
    "        \"선생님, 저랑 사귀실래요?\" 적극적인 어른 민주\n",
    "        \"꺼져\" 철벽 많은 급식 윤서\n",
    "\n",
    "        윤서는 학교에서 학생들에게 인기가 매우 많은 선생님이다.\n",
    "        어느 날, 윤서는 민주로부터 고백을 받게 된다.\n",
    "        하지만 윤서는 민주를 거절한다.\n",
    "\n",
    "        윤서와 민주, 두 사람의 아슬아슬한 만남은 계속된다.\n",
    "        \"\"\",\n",
    "        \"genre\": \"로맨스\",\n",
    "        \"actors\": [\"김민주\", \"박서준\", \"이도현\"]\n",
    "    },\n",
    "    {\n",
    "       \"synopsis\": \"\"\"\n",
    "        1919년, 3.1 운동 이후 봉오동 전투에서 승리한 독립군의 이야기를 그린 영화.\n",
    "        \"\"\",\n",
    "        \"genre\": \"액션\",\n",
    "        \"actors\": [\"유해진\", \"류준열\", \"조우진\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 가중치\n",
    "genre_weight = 3\n",
    "actor_weight = 3\n",
    "\n",
    "# 각 영화별 전처리된 토큰 리스트 생성 (2차원 배열)\n",
    "documents_tokens = [\n",
    "    preprocess_synopsis(movie[\"synopsis\"], movie[\"genre\"], movie[\"actors\"], genre_weight, actor_weight)\n",
    "    for movie in movies\n",
    "]\n",
    "\n",
    "# Word2Vec 하이퍼파라미터\n",
    "vector_size = 100  # 임베딩 벡터 차원\n",
    "window = 5  # 윈도우 크기 (주변 단어 범위)\n",
    "min_counts = [1, 2, 3]  # 최소 단어 빈도 (변경하면서 실험)\n",
    "sg_values = [0, 1]  # 0: CBOW, 1: Skip-gram\n",
    "\n",
    "def calculate_movie_vector(model, tokens):\n",
    "    \"\"\"\n",
    "    영화별 토큰들의 평균 벡터를 계산하는 함수.\n",
    "    \n",
    "    Args:\n",
    "      model: Word2Vec 모델\n",
    "      tokens: 영화별 토큰 리스트\n",
    "      \n",
    "    Returns:\n",
    "      영화 벡터 (NumPy 배열)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_vectors = []\n",
    "    for token in tokens:\n",
    "      if token in model.wv:\n",
    "        word_vectors.append(model.wv[token])\n",
    "        \n",
    "    if len(word_vectors) > 0:\n",
    "      return np.mean(word_vectors, axis=0) # 모든 단어 벡터의 평균\n",
    "    else:\n",
    "      return np.zeros(model.vector_size) # 빈 벡터 반환 (해당 영화에 모델 vocabulary에 있는 단어가 없는 경우)\n",
    "    \n",
    "\n",
    "# 여러 min_count와 sg 값에 대해 실험\n",
    "for min_count in min_counts:\n",
    "    for sg in sg_values:\n",
    "        print(f\"\\n--- min_count: {min_count}, sg: {sg} ({'CBOW' if sg == 0 else 'Skip-gram'}) ---\")\n",
    "\n",
    "        # Word2Vec 모델 학습\n",
    "        model = Word2Vec(\n",
    "            sentences=documents_tokens,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            sg=sg,  # 0: CBOW, 1: Skip-gram\n",
    "            workers=4,  # 병렬 처리 스레드 수\n",
    "        )\n",
    "\n",
    "        # 영화별 토큰 평균 벡터 계산\n",
    "        movie_vectors = [calculate_movie_vector(model, tokens) for tokens in documents_tokens]\n",
    "\n",
    "        # 영화 간 코사인 유사도 계산\n",
    "        similarity_matrix = cosine_similarity(movie_vectors)\n",
    "        print(\"코사인 유사도 행렬:\\n\", similarity_matrix)\n",
    "\n",
    "        # 0번 영화와 다른 영화 간의 유사도 확인\n",
    "        print(f\"0번 영화와 다른 영화 간 유사도: {similarity_matrix[0][1:]}\")\n",
    "```\n",
    "\n",
    "**설명:**\n",
    "\n",
    "1.  **`Word2Vec` 모델 학습:**\n",
    "    *   `sentences`: 토큰화된 문서 리스트(2차원 배열)를 입력으로 받습니다.\n",
    "    *   `vector_size`: 임베딩 벡터의 차원을 지정합니다.\n",
    "    *   `window`: 윈도우 크기(주변 단어 고려 범위)를 지정합니다.\n",
    "    *   `min_count`: 지정된 빈도보다 적게 나타나는 단어는 무시합니다.\n",
    "    *   `sg`: 0이면 CBOW, 1이면 Skip-gram 모델을 사용합니다.\n",
    "    *   `workers`: 학습에 사용할 스레드 수를 지정합니다.\n",
    "\n",
    "2.  **`calculate_movie_vector` 함수 :**\n",
    "    *   `word2vec.wv[token]`: Word2Vec 모델의 `wv` 속성을 사용하여 각 토큰의 임베딩 벡터를 가져옵니다.\n",
    "    *    `word_vectors` 리스트에 각 단어 벡터를 추가한다.\n",
    "    *   `np.mean(word_vectors, axis=0)`: 영화에 속한 모든 단어 벡터의 평균을 계산하여 영화 벡터를 생성합니다.  `axis=0`은 열(column) 방향으로 평균을 계산하라는 의미입니다.\n",
    "    *   `model.wv`에 없는 단어는 무시합니다.\n",
    "    *   해당 영화의 어떤 단어도 `model.wv`에 없다면 0으로 채워진 벡터를 반환합니다.\n",
    "\n",
    "3.  **영화별 토큰 평균 벡터 계산:**\n",
    "    *   각 영화의 토큰 리스트에 대해 `calculate_movie_vector`함수를 호출하여 영화 벡터를 계산합니다.\n",
    "\n",
    "4.  **코사인 유사도 계산:**\n",
    "    *   `cosine_similarity` 함수를 사용하여 영화 벡터 간의 코사인 유사도 행렬을 계산합니다.\n",
    "\n",
    "5.  **`min_count`와 `sg` 변경:**\n",
    "    *   `min_count`와 `sg` 값을 변경하면서 Word2Vec 모델을 학습시키고, 각 경우에 대한 코사인 유사도 행렬을 출력합니다.\n",
    "\n",
    "**결과 해석:**\n",
    "\n",
    "*   `min_count`를 늘리면, 더 자주 등장하는 단어들만 고려하게 되므로, 덜 중요한 단어의 영향을 줄일 수 있습니다. 하지만 너무 높게 설정하면 중요한 단어도 제외될 수 있습니다.\n",
    "*   `sg` 값을 변경하면 CBOW와 Skip-gram 모델 간의 성능 차이를 비교할 수 있습니다.\n",
    "*   코사인 유사도 행렬을 통해 영화 간의 유사도를 확인할 수 있습니다. 1에 가까울수록 유사하고, 0에 가까울수록 관련이 없습니다.\n",
    "\n",
    "**주의:**\n",
    "\n",
    "*   Word2Vec은 단어의 의미를 학습하지만, 문맥(context)을 완벽하게 이해하지는 못합니다.\n",
    "*   작은 데이터셋에서는 Word2Vec의 성능이 제한적일 수 있습니다. 더 큰 코퍼스(corpus)로 사전 학습된(pre-trained) Word2Vec 모델을 사용하는 것이 좋습니다. (예: Google News Vectors)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
